Computational Experiments & Code Changes:
[common]
  * Compare freq stats ACROSS studies; only difference is training set; see the impact on freq distribution
  * brady_etal_2005: implement kitterle-like cross-experiment stats
  
  * Find previously mentioned datasets
  
  * Compare ffts between targets and non-targets... or across conditions?
  * uber: do kitterle with theta=45, christman with theta=135
  * uber: do kitterle with theta=0, christman with theta=90
  
[31x13]:
  * Migrate to "common_args" model
  * Implement "uber"
  
[34x25]:
  * Get face recognition to work  (separate into different directory?)
  * Implement "uber"
  
  
  
Don't use datafile name in the AC hash; it contains the task, which only applies to the P.
But DO include options which, unfortunately, may have to do with the task...

Fix std on the _pct plot


% Also compare different kitterle pairs for different #s of thetas


Possible studies:
* ATTENTION / DEEP LEARNING
- model with specialized images more like attentional gain
- model with generic features (learn on all images) weird;
  attention for classification, but generic features
  we can make more sense:
  1. Train on non-specific set of images, to some criterion *across all images*
  2. Connect up classifier units, but *allow encoding weights to change*
    - this IS deep learning, with basin initialized based on generic features, rather than randomly

  We can also do deep-learning versions of this model, to see what happens:
  A. train autoencoder (850px input) with symmetric receptive fields
    1. connect hidden units (425px) of that autoencoder to one with asymmetric connections (let all weights be learned); note that we'll have a reduced map...
    NOTE: can follow principles of Amir et al paper
  B. Train autoencoder with asymemtric receptive fields
    1. connect hidden units with ones with asymmetric connections
    NOTE: can follow Amir principles
    NOTE: can see if the asymmetries build on each other
    

* DEVELOPMENT
- Reggia paper showed that asymmetric frequency filtering can develop from different types of input
- Did reggia show that RH learns more slowly than LH?

  What if we have wider connectivity, but impose a sparsity constraint, or some mechanism
  for pruning connections down to a set number (maybe in half?).  Look in the literature  for 
  info about horizontal connections and development
  
  Expectation: in order to code for LSF, RH will prune more local connections; LH 
  will prune more narrow connections

% Test what happens when we turn on non-linearity at all layers


NEW GOAL: Anthropology

EMAIL Katerina about meeting, to talk about science & ethics
  about getting involved in anthropology
EMAIL PASCAL about meeting, to talk about science & ethics